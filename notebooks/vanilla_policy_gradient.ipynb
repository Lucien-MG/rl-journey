{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENVIRONMENT_ID = \"CartPole-v1\" # \"LunarLander-v2\"\n",
    "RECORD_PATH = \"../videos/reinforce/\" + ENVIRONMENT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch RL Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributions as dist\n",
    "\n",
    "class Stochastic(nn.Module):\n",
    "    def __init__(self, num_actions, distribution: torch.distributions.distribution.Distribution) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_actions = num_actions\n",
    "        self.distribution = distribution\n",
    "    \n",
    "    def forward(self, states):\n",
    "        probs = torch.full((states.shape[0], self.num_actions), fill_value=1/self.num_actions)\n",
    "        return probs\n",
    "\n",
    "    def policy(self, state):\n",
    "        values = self.forward(state)\n",
    "        if hasattr(self.distribution, 'logits'):\n",
    "            policy = self.distribution(logits=values)\n",
    "        else:\n",
    "            policy = self.distribution(values[0], values[1])\n",
    "        return policy\n",
    "    \n",
    "    def action(self, state: torch.Tensor):\n",
    "        return self.policy(state).sample()\n",
    "\n",
    "\n",
    "class Reinforce(torch.nn.Module):\n",
    "    def __init__(self, model: torch.nn.Module, distribution: torch.distributions.distribution.Distribution) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = model\n",
    "        self.distribution = distribution\n",
    "\n",
    "    def forward(self, state: torch.Tensor):\n",
    "        values = self.model(state)\n",
    "        return values\n",
    "    \n",
    "    def policy(self, state: torch.Tensor):\n",
    "        values = self.forward(state)\n",
    "        if hasattr(self.distribution, 'logits'):\n",
    "            policy = self.distribution(logits=values)\n",
    "        else:\n",
    "            policy = self.distribution(values[0], values[1])\n",
    "        return policy\n",
    "\n",
    "    def action(self, state: torch.Tensor):\n",
    "        return self.policy(state).sample()\n",
    "\n",
    "    def loss(self, state_batch: torch.Tensor, action_batch: torch.Tensor, reward_batch: torch.Tensor, done_batch: torch.Tensor):\n",
    "        distribution = self.policy(state_batch)\n",
    "        entropy = distribution.entropy().mean()\n",
    "        logp = distribution.log_prob(action_batch)\n",
    "        loss = -(logp * reward_batch).mean()\n",
    "        return loss, entropy\n",
    "\n",
    "\n",
    "class ActionBuffer():\n",
    "    def __init__(self, gamma) -> None:\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.state_buffer = []\n",
    "        self.action_buffer = []\n",
    "        self.reward_buffer = []\n",
    "        self.done_buffer = []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.state_buffer)\n",
    "    \n",
    "    def push(self, state, action, reward, done):\n",
    "        self.state_buffer.append(state)\n",
    "        self.action_buffer.append(action)\n",
    "        self.reward_buffer.append(reward)\n",
    "        self.done_buffer.append(done)\n",
    "    \n",
    "    def flush(self):\n",
    "        for i in reversed(range(self.__len__())):\n",
    "            self.reward_buffer[i - 1] = self.reward_buffer[i - 1] + (self.gamma * (self.reward_buffer[i] * (not self.done_buffer[i - 1])))\n",
    "        \n",
    "        state_batch = torch.cat(self.state_buffer)\n",
    "        action_batch = torch.FloatTensor(self.action_buffer)\n",
    "        reward_batch = torch.FloatTensor(self.reward_buffer)\n",
    "        done_batch = torch.BoolTensor(self.done_buffer)\n",
    "\n",
    "        self.__init__(self.gamma)\n",
    "\n",
    "        return state_batch, action_batch, reward_batch, done_batch\n",
    "\n",
    "def get_prob_from_pred(pred_batch, action_batch):\n",
    "    return pred_batch.gather(dim=1,index=action_batch.long().view(-1,1)).squeeze()\n",
    "\n",
    "def categorical_policy(probs):\n",
    "    distribution = dist.Categorical(logits=probs)\n",
    "    return distribution\n",
    "\n",
    "def categorical_action(probs):\n",
    "    distribution = dist.Categorical(logits=probs)\n",
    "    return distribution.sample()\n",
    "\n",
    "def normal_policy(mu, sigma):\n",
    "    distribution = dist.Normal(mu, sigma)\n",
    "    return distribution\n",
    "\n",
    "def normal_action(mu, sigma):\n",
    "    distribution = dist.Normal(mu, sigma)\n",
    "    return distribution.sample()\n",
    "\n",
    "def ascent_log_loss(action_values_batch, action_batch, reward_batch):\n",
    "    distribution = dist.Categorical(logits=action_values_batch)\n",
    "    logp = distribution.log_prob(action_batch)\n",
    "    loss = -(logp * reward_batch).mean()\n",
    "    return loss\n",
    "\n",
    "def entropy_loss(action_values_batch, beta=0.1):\n",
    "    p = torch.softmax(action_values_batch, dim=1)\n",
    "    log_p = torch.log_softmax(action_values_batch, dim=1)\n",
    "    entropy = -1 * torch.mean(torch.sum(p * log_p, dim=1), dim=0)\n",
    "    entropy_bonus = -1 * beta * entropy\n",
    "    return entropy_bonus\n",
    "\n",
    "# make loss function whose gradient, for the right data, is policy gradient\n",
    "def log_loss(action_values, reward_to_go):\n",
    "    error = torch.log(action_values) * reward_to_go\n",
    "    return -torch.mean(error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gym Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers import RecordVideo\n",
    "\n",
    "# Play episode\n",
    "def play_episode(env, agent, record_path=None):\n",
    "    if record_path:\n",
    "        env = RecordVideo(env, video_folder=record_path, new_step_api=True)\n",
    "\n",
    "    cumulative_reward = 0\n",
    "\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        torch_state = torch.FloatTensor(observation).unsqueeze(dim=0)\n",
    "        action = agent.action(torch_state)\n",
    "\n",
    "        observation, reward, terminated, truncarted, info = env.step(action.item())\n",
    "\n",
    "        done = terminated or truncarted\n",
    "\n",
    "        cumulative_reward += reward\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    return cumulative_reward\n",
    "\n",
    "def evaluate_agent(env, agent, num_episodes=100):\n",
    "    reward = 0\n",
    "    for episode in range(num_episodes):\n",
    "        reward += play_episode(env, agent)\n",
    "    return reward / num_episodes\n",
    "\n",
    "def train_epoch(env, agent, optimizer, gamma=0.99, num_episode=32):\n",
    "    memory = ActionBuffer(gamma)\n",
    "\n",
    "    cumulative_rewards = []\n",
    "    episodes_length = []\n",
    "\n",
    "    for episode in range(num_episode):\n",
    "        observation = env.reset()\n",
    "        done = False\n",
    "\n",
    "        tt_reward = 0\n",
    "        ep_length = 0\n",
    "\n",
    "        while not done:\n",
    "            torch_state = torch.as_tensor(observation).unsqueeze(dim=0)\n",
    "            torch_action = agent.action(torch_state)\n",
    "\n",
    "            next_observation, reward, terminated, truncarted, info = env.step(torch_action.item())\n",
    "\n",
    "            done = terminated or truncarted\n",
    "            memory.push(torch_state, torch_action, reward, done)\n",
    "\n",
    "            observation = next_observation\n",
    "\n",
    "            tt_reward += reward\n",
    "            ep_length += 1\n",
    "\n",
    "        cumulative_rewards.append(tt_reward)\n",
    "        episodes_length.append(ep_length)\n",
    "\n",
    "    state_batch, action_batch, reward_batch, done_batch = memory.flush()\n",
    "\n",
    "    loss, entropy = agent.loss(state_batch, action_batch, reward_batch, done_batch)\n",
    "    loss = loss + entropy\n",
    "\n",
    "    #action_values_batch = agent(state_batch)\n",
    "\n",
    "    #loss = ascent_log_loss(action_values_batch, action_batch, reward_batch)\n",
    "    #loss += entropy_loss(action_values_batch)\n",
    "\n",
    "    #action_values_batch = torch.softmax(action_values_batch, dim=1)\n",
    "    #action_value_batch = get_prob_from_pred(action_values_batch, action_batch)\n",
    "    #loss = log_loss(action_value_batch, reward_batch)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return cumulative_rewards, loss, entropy, episodes_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters:\n",
    "ALPHA = 1e-2\n",
    "GAMMA = 0.99\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "NB_EPOCH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(ENVIRONMENT_ID, new_step_api=True)\n",
    "\n",
    "# Infos Env\n",
    "observation_space_size = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter(comment=f'_PG_CP_Gamma={GAMMA},'\n",
    "                                        f'LR={ALPHA},'\n",
    "                                        f'BS={BATCH_SIZE}'\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "            nn.Linear(in_features=observation_space_size, out_features=16, bias=True),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(in_features=16, out_features=16, bias=True),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(in_features=16, out_features=num_actions, bias=True),\n",
    "            nn.Identity()\n",
    "        )\n",
    "\n",
    "random_agent = Stochastic(num_actions=num_actions, distribution=torch.distributions.Categorical)\n",
    "agent = Reinforce(model=model, distribution=torch.distributions.Categorical)\n",
    "\n",
    "optimizer = torch.optim.Adam(params=agent.parameters(), lr=ALPHA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward, agent: 22.37 random, 22.69\n"
     ]
    }
   ],
   "source": [
    "reward_random = evaluate_agent(env, agent=random_agent, num_episodes=100)\n",
    "reward_agent = evaluate_agent(env, agent=agent, num_episodes=100)\n",
    "\n",
    "print(\"Mean Reward, agent:\", reward_agent, \"random,\", reward_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 reward: 20.5625\n",
      "Epoch 1 reward: 19.625\n",
      "Epoch 2 reward: 26.4375\n",
      "Epoch 3 reward: 25.15625\n",
      "Epoch 4 reward: 23.375\n",
      "Epoch 5 reward: 27.890625\n",
      "Epoch 6 reward: 30.453125\n",
      "Epoch 7 reward: 29.53125\n",
      "Epoch 8 reward: 32.296875\n",
      "Epoch 9 reward: 34.34375\n",
      "Epoch 10 reward: 44.515625\n",
      "Epoch 11 reward: 39.71875\n",
      "Epoch 12 reward: 54.0625\n",
      "Epoch 13 reward: 50.65625\n",
      "Epoch 14 reward: 61.8125\n",
      "Epoch 15 reward: 63.125\n",
      "Epoch 16 reward: 78.359375\n",
      "Epoch 17 reward: 79.09375\n",
      "Epoch 18 reward: 85.546875\n",
      "Epoch 19 reward: 91.0625\n",
      "Epoch 20 reward: 97.875\n",
      "Epoch 21 reward: 115.4375\n",
      "Epoch 22 reward: 122.046875\n",
      "Epoch 23 reward: 148.4375\n",
      "Epoch 24 reward: 165.1875\n",
      "Epoch 25 reward: 180.953125\n",
      "Epoch 26 reward: 229.609375\n",
      "Epoch 27 reward: 307.671875\n",
      "Epoch 28 reward: 374.546875\n",
      "Epoch 29 reward: 394.4375\n",
      "Epoch 30 reward: 424.515625\n",
      "Epoch 31 reward: 457.625\n",
      "Epoch 32 reward: 482.578125\n",
      "Epoch 33 reward: 496.78125\n",
      "Epoch 34 reward: 494.359375\n",
      "Epoch 35 reward: 495.703125\n",
      "Epoch 36 reward: 482.734375\n",
      "Epoch 37 reward: 478.75\n",
      "Epoch 38 reward: 472.390625\n",
      "Epoch 39 reward: 475.078125\n",
      "Epoch 40 reward: 476.296875\n",
      "Epoch 41 reward: 462.109375\n",
      "Epoch 42 reward: 472.4375\n",
      "Epoch 43 reward: 467.0\n",
      "Epoch 44 reward: 483.125\n",
      "Epoch 45 reward: 474.3125\n",
      "Epoch 46 reward: 468.953125\n",
      "Epoch 47 reward: 440.46875\n",
      "Epoch 48 reward: 443.484375\n",
      "Epoch 49 reward: 428.078125\n",
      "Epoch 50 reward: 268.34375\n",
      "Epoch 51 reward: 151.890625\n",
      "Epoch 52 reward: 144.328125\n",
      "Epoch 53 reward: 142.3125\n",
      "Epoch 54 reward: 144.640625\n",
      "Epoch 55 reward: 144.171875\n",
      "Epoch 56 reward: 146.0625\n",
      "Epoch 57 reward: 145.84375\n",
      "Epoch 58 reward: 146.140625\n",
      "Epoch 59 reward: 151.296875\n",
      "Epoch 60 reward: 148.640625\n",
      "Epoch 61 reward: 155.75\n",
      "Epoch 62 reward: 161.09375\n",
      "Epoch 63 reward: 171.28125\n",
      "Epoch 64 reward: 185.390625\n",
      "Epoch 65 reward: 211.953125\n",
      "Epoch 66 reward: 259.9375\n",
      "Epoch 67 reward: 334.0\n",
      "Epoch 68 reward: 445.078125\n",
      "Epoch 69 reward: 498.828125\n",
      "Epoch 70 reward: 500.0\n",
      "Epoch 71 reward: 500.0\n",
      "Epoch 72 reward: 500.0\n",
      "Epoch 73 reward: 500.0\n",
      "Epoch 74 reward: 500.0\n",
      "Epoch 75 reward: 500.0\n",
      "Epoch 76 reward: 500.0\n",
      "Epoch 77 reward: 500.0\n",
      "Epoch 78 reward: 500.0\n",
      "Epoch 79 reward: 500.0\n",
      "Epoch 80 reward: 500.0\n",
      "Epoch 81 reward: 500.0\n",
      "Epoch 82 reward: 500.0\n",
      "Epoch 83 reward: 500.0\n",
      "Epoch 84 reward: 500.0\n",
      "Epoch 85 reward: 500.0\n",
      "Epoch 86 reward: 500.0\n",
      "Epoch 87 reward: 500.0\n",
      "Epoch 88 reward: 500.0\n",
      "Epoch 89 reward: 499.265625\n",
      "Epoch 90 reward: 500.0\n",
      "Epoch 91 reward: 500.0\n",
      "Epoch 92 reward: 499.265625\n",
      "Epoch 93 reward: 497.078125\n",
      "Epoch 94 reward: 499.3125\n",
      "Epoch 95 reward: 496.90625\n",
      "Epoch 96 reward: 497.28125\n",
      "Epoch 97 reward: 495.171875\n",
      "Epoch 98 reward: 491.65625\n",
      "Epoch 99 reward: 471.46875\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(NB_EPOCH):\n",
    "    rewards, loss, entropy, episodes_length = train_epoch(env, agent=agent, optimizer=optimizer, gamma=GAMMA, num_episode=BATCH_SIZE)\n",
    "\n",
    "    print(\"Epoch\", epoch, \"reward:\", torch.FloatTensor(rewards).mean().item())\n",
    "\n",
    "    writer.add_scalar(\"mean_reward\", torch.FloatTensor(rewards).mean(), epoch)\n",
    "    writer.add_scalar(\"mean_length\", torch.FloatTensor(episodes_length).mean(), epoch)\n",
    "    writer.add_scalar(\"loss\", loss, epoch)\n",
    "    writer.add_scalar(\"entropy\", entropy, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward, agent: 460.39 random, 22.08\n"
     ]
    }
   ],
   "source": [
    "reward_random = evaluate_agent(env, agent=random_agent, num_episodes=100)\n",
    "reward_agent = evaluate_agent(env, agent=agent, num_episodes=100)\n",
    "\n",
    "print(\"Mean Reward, agent:\", reward_agent, \"random,\", reward_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lulu/Worskpace/rl-journey/.venv/lib/python3.10/site-packages/gym/wrappers/record_video.py:78: UserWarning: \u001b[33mWARN: Overwriting existing videos at /Users/lulu/Worskpace/rl-journey/videos/reinforce/CartPole-v1 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/lulu/Worskpace/rl-journey/.venv/lib/python3.10/site-packages/gym/wrappers/monitoring/video_recorder.py:78: DeprecationWarning: \u001b[33mWARN: Recording ability for environment CartPole-v1 initialized with `render_mode=None` is marked as deprecated and will be removed in the future.\u001b[0m\n",
      "  logger.deprecation(\n",
      "/Users/lulu/Worskpace/rl-journey/.venv/lib/python3.10/site-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
      "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: 454.0\n"
     ]
    }
   ],
   "source": [
    "reward = play_episode(env, agent=agent, record_path=RECORD_PATH)\n",
    "\n",
    "print(\"Reward:\", reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "51ae4b48cd667d26a35c5140fc7a1db972b291d20a1bee8d89eeb2339d6da7a3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
