{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENVIRONMENT_ID = \"Pendulum-v1\" # \"MountainCar-v0\" # \"MountainCarContinuous-v0\" # \"Pendulum-v1\" # \"CartPole-v1\" # \"LunarLander-v2\"\n",
    "RECORD_PATH = \"../videos/reinforce/\" + ENVIRONMENT_ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Torch RL Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributions as dist\n",
    "\n",
    "class Stochastic(nn.Module):\n",
    "\n",
    "    def __init__(self, num_actions, distribution: torch.distributions.distribution.Distribution) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_actions = num_actions\n",
    "        self.distribution = distribution\n",
    "    \n",
    "    def forward(self, states):\n",
    "        probs = torch.full((states.shape[0], self.num_actions), fill_value=1/self.num_actions)\n",
    "        return probs\n",
    "\n",
    "    def policy(self, state):\n",
    "        values = self.forward(state)\n",
    "        policy = self.distribution(logits=values)\n",
    "        return policy\n",
    "    \n",
    "    def action(self, state: torch.Tensor):\n",
    "        action = self.policy(state).sample()\n",
    "        return action\n",
    "\n",
    "\n",
    "class Reinforce(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, model: torch.nn.Module, distribution: torch.distributions.distribution.Distribution) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = model\n",
    "        self.distribution = distribution\n",
    "\n",
    "    def forward(self, state: torch.Tensor):\n",
    "        values = self.model(state)\n",
    "        return values\n",
    "    \n",
    "    def policy(self, state: torch.Tensor):\n",
    "        values = self.forward(state)\n",
    "        if hasattr(self.distribution, 'logits'):\n",
    "            policy = self.distribution(logits=values)\n",
    "        else:\n",
    "            policy = self.distribution(values, 0.1)\n",
    "        return policy\n",
    "\n",
    "    def action(self, state: torch.Tensor):\n",
    "        action = self.policy(state).sample()\n",
    "        return action\n",
    "\n",
    "    def loss(self, state_batch: torch.Tensor, action_batch: torch.Tensor, reward_batch: torch.Tensor, done_batch: torch.Tensor):\n",
    "        distribution = self.policy(state_batch)\n",
    "        entropy = distribution.entropy().mean()\n",
    "        logp = distribution.log_prob(action_batch)\n",
    "        loss = -(logp * reward_batch).mean()\n",
    "        return loss, entropy\n",
    "\n",
    "\n",
    "class ActionBuffer():\n",
    "    def __init__(self, gamma) -> None:\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "\n",
    "        self.state_buffer = []\n",
    "        self.action_buffer = []\n",
    "        self.reward_buffer = []\n",
    "        self.done_buffer = []\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.state_buffer)\n",
    "    \n",
    "    def push(self, state, action, reward, done):\n",
    "        self.state_buffer.append(state)\n",
    "        self.action_buffer.append(action)\n",
    "        self.reward_buffer.append(reward)\n",
    "        self.done_buffer.append(done)\n",
    "    \n",
    "    def flush(self):\n",
    "        for i in reversed(range(self.__len__())):\n",
    "            self.reward_buffer[i - 1] = self.reward_buffer[i - 1] + (self.gamma * (self.reward_buffer[i] * (not self.done_buffer[i - 1])))\n",
    "        \n",
    "        state_batch = torch.cat(self.state_buffer)\n",
    "        action_batch = torch.FloatTensor(self.action_buffer)\n",
    "        reward_batch = torch.FloatTensor(self.reward_buffer)\n",
    "        done_batch = torch.BoolTensor(self.done_buffer)\n",
    "\n",
    "        self.__init__(self.gamma)\n",
    "\n",
    "        return state_batch, action_batch, reward_batch, done_batch\n",
    "\n",
    "def get_prob_from_pred(pred_batch, action_batch):\n",
    "    return pred_batch.gather(dim=1,index=action_batch.long().view(-1,1)).squeeze()\n",
    "\n",
    "def ascent_log_loss(action_values_batch, action_batch, reward_batch):\n",
    "    distribution = dist.Categorical(logits=action_values_batch)\n",
    "    logp = distribution.log_prob(action_batch)\n",
    "    loss = -(logp * reward_batch).mean()\n",
    "    return loss\n",
    "\n",
    "def entropy_loss(action_values_batch, beta=0.1):\n",
    "    p = torch.softmax(action_values_batch, dim=1)\n",
    "    log_p = torch.log_softmax(action_values_batch, dim=1)\n",
    "    entropy = -1 * torch.mean(torch.sum(p * log_p, dim=1), dim=0)\n",
    "    entropy_bonus = -1 * beta * entropy\n",
    "    return entropy_bonus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gym Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import exp\n",
    "from gym.wrappers import RecordVideo\n",
    "\n",
    "# Play episode\n",
    "def play_episode(env, agent, record_path=None):\n",
    "    if record_path:\n",
    "        env = RecordVideo(env, video_folder=record_path, new_step_api=True)\n",
    "\n",
    "    cumulative_reward = 0\n",
    "\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        torch_state = torch.FloatTensor(observation)\n",
    "        action = agent.action(torch_state)\n",
    "\n",
    "        observation, reward, terminated, truncarted, info = env.step(action.numpy())\n",
    "        done = terminated or truncarted\n",
    "\n",
    "        cumulative_reward += reward\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    return cumulative_reward\n",
    "\n",
    "def evaluate_agent(env, agent, num_episodes=100):\n",
    "    reward = 0\n",
    "    for episode in range(num_episodes):\n",
    "        reward += play_episode(env, agent)\n",
    "    return reward / num_episodes\n",
    "\n",
    "def train_epoch(env, agent, optimizer, gamma=0.99, num_episode=32):\n",
    "    memory = ActionBuffer(gamma)\n",
    "\n",
    "    cumulative_rewards = []\n",
    "    episodes_length = []\n",
    "\n",
    "    for episode in range(num_episode):\n",
    "        observation = env.reset()\n",
    "        done = False\n",
    "\n",
    "        tt_reward = 0\n",
    "        ep_length = 0\n",
    "\n",
    "        while not done:\n",
    "            print(\"obs:\", observation.shape)\n",
    "            torch_state = torch.as_tensor(observation).unsqueeze(0)\n",
    "            print(\"Torch obs\", torch_state.shape)\n",
    "            torch_action = agent.action(torch_state)\n",
    "\n",
    "            next_observation, reward, terminated, truncarted, info = env.step(torch_action.numpy())\n",
    "            done = terminated or truncarted\n",
    "\n",
    "            memory.push(torch_state, torch_action, reward, done)\n",
    "\n",
    "            observation = next_observation\n",
    "\n",
    "            tt_reward += reward \n",
    "            ep_length += 1\n",
    "\n",
    "        cumulative_rewards.append(tt_reward)\n",
    "        episodes_length.append(ep_length)\n",
    "\n",
    "    state_batch, action_batch, reward_batch, done_batch = memory.flush()\n",
    "\n",
    "    loss, entropy = agent.loss(state_batch, action_batch, reward_batch, done_batch)\n",
    "    loss = loss + 0.1 * entropy\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return cumulative_rewards, loss, entropy, episodes_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters:\n",
    "ALPHA = 1e-2\n",
    "GAMMA = 0.99\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "NB_EPOCH = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment Infos:\n",
      "  Observation Shape: (3,)\n",
      "  Number of actions: 1\n",
      "  Problem is continuous: True\n"
     ]
    }
   ],
   "source": [
    "# Create environment\n",
    "env = gym.make(ENVIRONMENT_ID, new_step_api=True)\n",
    "\n",
    "# Infos Env\n",
    "observation_space_size = env.observation_space.shape\n",
    "num_actions = env.action_space.n if type(env.action_space.sample()) == \"int\" else env.action_space.sample().shape[0]\n",
    "is_continuous = env.action_space.dtype == \"float32\"\n",
    "\n",
    "print(\"Environment Infos:\")\n",
    "print(\"  Observation Shape:\", observation_space_size)\n",
    "print(\"  Number of actions:\", num_actions)\n",
    "print(\"  Problem is continuous:\", is_continuous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter(comment=f'_PG_CP_Gamma={GAMMA},'\n",
    "                                f'LR={ALPHA},'\n",
    "                                f'BS={BATCH_SIZE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_continuous:\n",
    "    distribution = torch.distributions.Normal\n",
    "else:\n",
    "    distribution = torch.distributions.Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "empty(): argument 'size' must be tuple of ints, but found element of type tuple at pos 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/lulu/Worskpace/rl-journey/notebooks/vanilla_policy_gradient_v2.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lulu/Worskpace/rl-journey/notebooks/vanilla_policy_gradient_v2.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mSequential(\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/lulu/Worskpace/rl-journey/notebooks/vanilla_policy_gradient_v2.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m             nn\u001b[39m.\u001b[39;49mLinear(in_features\u001b[39m=\u001b[39;49mobservation_space_size, out_features\u001b[39m=\u001b[39;49m\u001b[39m16\u001b[39;49m, bias\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m),\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lulu/Worskpace/rl-journey/notebooks/vanilla_policy_gradient_v2.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m             nn\u001b[39m.\u001b[39mPReLU(),\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lulu/Worskpace/rl-journey/notebooks/vanilla_policy_gradient_v2.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m             nn\u001b[39m.\u001b[39mLinear(in_features\u001b[39m=\u001b[39m\u001b[39m16\u001b[39m, out_features\u001b[39m=\u001b[39m\u001b[39m16\u001b[39m, bias\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m),\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lulu/Worskpace/rl-journey/notebooks/vanilla_policy_gradient_v2.ipynb#X13sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m             nn\u001b[39m.\u001b[39mPReLU(),\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lulu/Worskpace/rl-journey/notebooks/vanilla_policy_gradient_v2.ipynb#X13sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m             nn\u001b[39m.\u001b[39mLinear(in_features\u001b[39m=\u001b[39m\u001b[39m16\u001b[39m, out_features\u001b[39m=\u001b[39mnum_actions, bias\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m),\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lulu/Worskpace/rl-journey/notebooks/vanilla_policy_gradient_v2.ipynb#X13sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m             nn\u001b[39m.\u001b[39mIdentity()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lulu/Worskpace/rl-journey/notebooks/vanilla_policy_gradient_v2.ipynb#X13sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lulu/Worskpace/rl-journey/notebooks/vanilla_policy_gradient_v2.ipynb#X13sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m random_agent \u001b[39m=\u001b[39m Stochastic(num_actions\u001b[39m=\u001b[39mnum_actions, distribution\u001b[39m=\u001b[39mdistribution)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lulu/Worskpace/rl-journey/notebooks/vanilla_policy_gradient_v2.ipynb#X13sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m agent \u001b[39m=\u001b[39m Reinforce(model\u001b[39m=\u001b[39mmodel, distribution\u001b[39m=\u001b[39mdistribution)\n",
      "File \u001b[0;32m~/Worskpace/rl-journey/.venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:96\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_features \u001b[39m=\u001b[39m in_features\n\u001b[1;32m     95\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout_features \u001b[39m=\u001b[39m out_features\n\u001b[0;32m---> 96\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight \u001b[39m=\u001b[39m Parameter(torch\u001b[39m.\u001b[39;49mempty((out_features, in_features), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfactory_kwargs))\n\u001b[1;32m     97\u001b[0m \u001b[39mif\u001b[39;00m bias:\n\u001b[1;32m     98\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39m=\u001b[39m Parameter(torch\u001b[39m.\u001b[39mempty(out_features, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfactory_kwargs))\n",
      "\u001b[0;31mTypeError\u001b[0m: empty(): argument 'size' must be tuple of ints, but found element of type tuple at pos 2"
     ]
    }
   ],
   "source": [
    "model = nn.Sequential(\n",
    "            nn.Linear(in_features=observation_space_size, out_features=16, bias=True),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(in_features=16, out_features=16, bias=True),\n",
    "            nn.PReLU(),\n",
    "            nn.Linear(in_features=16, out_features=num_actions, bias=True),\n",
    "            nn.Identity()\n",
    "        )\n",
    "\n",
    "random_agent = Stochastic(num_actions=num_actions, distribution=distribution)\n",
    "agent = Reinforce(model=model, distribution=distribution)\n",
    "\n",
    "optimizer = torch.optim.Adam(params=agent.parameters(), lr=ALPHA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lulu/Worskpace/rl-journey/.venv/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:165: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "/Users/lulu/Worskpace/rl-journey/.venv/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:260: UserWarning: \u001b[33mWARN: The reward returned by `step()` must be a float, int, np.integer or np.floating, actual type: <class 'numpy.ndarray'>\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward, agent: -1175.815305606989 random, [-1339.1133]\n"
     ]
    }
   ],
   "source": [
    "reward_random = evaluate_agent(env, agent=random_agent, num_episodes=100)\n",
    "reward_agent = evaluate_agent(env, agent=agent, num_episodes=100)\n",
    "\n",
    "print(\"Mean Reward, agent:\", reward_agent, \"random,\", reward_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs: (3,)\n",
      "Torch obs torch.Size([1, 3])\n",
      "obs: (3, 1)\n",
      "Torch obs torch.Size([1, 3, 1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (3x1 and 3x16)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/lulu/Worskpace/rl-journey/notebooks/vanilla_policy_gradient_v2.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lulu/Worskpace/rl-journey/notebooks/vanilla_policy_gradient_v2.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(NB_EPOCH):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/lulu/Worskpace/rl-journey/notebooks/vanilla_policy_gradient_v2.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     rewards, loss, entropy, episodes_length \u001b[39m=\u001b[39m train_epoch(env, agent\u001b[39m=\u001b[39;49magent, optimizer\u001b[39m=\u001b[39;49moptimizer, gamma\u001b[39m=\u001b[39;49mGAMMA, num_episode\u001b[39m=\u001b[39;49mBATCH_SIZE)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lulu/Worskpace/rl-journey/notebooks/vanilla_policy_gradient_v2.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mEpoch\u001b[39m\u001b[39m\"\u001b[39m, epoch, \u001b[39m\"\u001b[39m\u001b[39mreward:\u001b[39m\u001b[39m\"\u001b[39m, torch\u001b[39m.\u001b[39mFloatTensor(rewards)\u001b[39m.\u001b[39mmean()\u001b[39m.\u001b[39mitem())\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/lulu/Worskpace/rl-journey/notebooks/vanilla_policy_gradient_v2.ipynb#X15sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     writer\u001b[39m.\u001b[39madd_scalar(\u001b[39m\"\u001b[39m\u001b[39mmean_reward\u001b[39m\u001b[39m\"\u001b[39m, torch\u001b[39m.\u001b[39mFloatTensor(rewards)\u001b[39m.\u001b[39mmean(), epoch)\n",
      "\u001b[1;32m/Users/lulu/Worskpace/rl-journey/notebooks/vanilla_policy_gradient_v2.ipynb Cell 14\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(env, agent, optimizer, gamma, num_episode)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lulu/Worskpace/rl-journey/notebooks/vanilla_policy_gradient_v2.ipynb#X15sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m torch_state \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mas_tensor(observation)\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lulu/Worskpace/rl-journey/notebooks/vanilla_policy_gradient_v2.ipynb#X15sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTorch obs\u001b[39m\u001b[39m\"\u001b[39m, torch_state\u001b[39m.\u001b[39mshape)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/lulu/Worskpace/rl-journey/notebooks/vanilla_policy_gradient_v2.ipynb#X15sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m torch_action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49maction(torch_state)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lulu/Worskpace/rl-journey/notebooks/vanilla_policy_gradient_v2.ipynb#X15sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m next_observation, reward, terminated, truncarted, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(torch_action\u001b[39m.\u001b[39mnumpy())\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lulu/Worskpace/rl-journey/notebooks/vanilla_policy_gradient_v2.ipynb#X15sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m done \u001b[39m=\u001b[39m terminated \u001b[39mor\u001b[39;00m truncarted\n",
      "\u001b[1;32m/Users/lulu/Worskpace/rl-journey/notebooks/vanilla_policy_gradient_v2.ipynb Cell 14\u001b[0m in \u001b[0;36mReinforce.action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lulu/Worskpace/rl-journey/notebooks/vanilla_policy_gradient_v2.ipynb#X15sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39maction\u001b[39m(\u001b[39mself\u001b[39m, state: torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/lulu/Worskpace/rl-journey/notebooks/vanilla_policy_gradient_v2.ipynb#X15sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m     action \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy(state)\u001b[39m.\u001b[39msample()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lulu/Worskpace/rl-journey/notebooks/vanilla_policy_gradient_v2.ipynb#X15sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m action\n",
      "\u001b[1;32m/Users/lulu/Worskpace/rl-journey/notebooks/vanilla_policy_gradient_v2.ipynb Cell 14\u001b[0m in \u001b[0;36mReinforce.policy\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lulu/Worskpace/rl-journey/notebooks/vanilla_policy_gradient_v2.ipynb#X15sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpolicy\u001b[39m(\u001b[39mself\u001b[39m, state: torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/lulu/Worskpace/rl-journey/notebooks/vanilla_policy_gradient_v2.ipynb#X15sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(state)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lulu/Worskpace/rl-journey/notebooks/vanilla_policy_gradient_v2.ipynb#X15sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribution, \u001b[39m'\u001b[39m\u001b[39mlogits\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lulu/Worskpace/rl-journey/notebooks/vanilla_policy_gradient_v2.ipynb#X15sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m         policy \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribution(logits\u001b[39m=\u001b[39mvalues)\n",
      "\u001b[1;32m/Users/lulu/Worskpace/rl-journey/notebooks/vanilla_policy_gradient_v2.ipynb Cell 14\u001b[0m in \u001b[0;36mReinforce.forward\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lulu/Worskpace/rl-journey/notebooks/vanilla_policy_gradient_v2.ipynb#X15sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, state: torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/lulu/Worskpace/rl-journey/notebooks/vanilla_policy_gradient_v2.ipynb#X15sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(state)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/lulu/Worskpace/rl-journey/notebooks/vanilla_policy_gradient_v2.ipynb#X15sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m values\n",
      "File \u001b[0;32m~/Worskpace/rl-journey/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Worskpace/rl-journey/.venv/lib/python3.10/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/Worskpace/rl-journey/.venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Worskpace/rl-journey/.venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (3x1 and 3x16)"
     ]
    }
   ],
   "source": [
    "for epoch in range(NB_EPOCH):\n",
    "    rewards, loss, entropy, episodes_length = train_epoch(env, agent=agent, optimizer=optimizer, gamma=GAMMA, num_episode=BATCH_SIZE)\n",
    "\n",
    "    print(\"Epoch\", epoch, \"reward:\", torch.FloatTensor(rewards).mean().item())\n",
    "\n",
    "    writer.add_scalar(\"mean_reward\", torch.FloatTensor(rewards).mean(), epoch)\n",
    "    writer.add_scalar(\"mean_length\", torch.FloatTensor(episodes_length).mean(), epoch)\n",
    "    writer.add_scalar(\"loss\", loss, epoch)\n",
    "    writer.add_scalar(\"entropy\", entropy, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Reward, agent: -200.0 random, -200.0\n"
     ]
    }
   ],
   "source": [
    "reward_random = evaluate_agent(env, agent=random_agent, num_episodes=100)\n",
    "reward_agent = evaluate_agent(env, agent=agent, num_episodes=100)\n",
    "\n",
    "print(\"Mean Reward, agent:\", reward_agent, \"random,\", reward_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lulu/Worskpace/rl-journey/.venv/lib/python3.10/site-packages/gym/wrappers/record_video.py:78: UserWarning: \u001b[33mWARN: Overwriting existing videos at /Users/lulu/Worskpace/rl-journey/videos/reinforce/MountainCar-v0 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n",
      "/Users/lulu/Worskpace/rl-journey/.venv/lib/python3.10/site-packages/gym/wrappers/monitoring/video_recorder.py:78: DeprecationWarning: \u001b[33mWARN: Recording ability for environment MountainCar-v0 initialized with `render_mode=None` is marked as deprecated and will be removed in the future.\u001b[0m\n",
      "  logger.deprecation(\n",
      "/Users/lulu/Worskpace/rl-journey/.venv/lib/python3.10/site-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
      "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
      "  deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: -200.0\n"
     ]
    }
   ],
   "source": [
    "reward = play_episode(env, agent=agent, record_path=RECORD_PATH)\n",
    "\n",
    "print(\"Reward:\", reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('.venv': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "51ae4b48cd667d26a35c5140fc7a1db972b291d20a1bee8d89eeb2339d6da7a3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
